### MLP Classifier

#### 20171181 최영빈

```python
from sklearn.neural_network import MLPClassifier
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
```

-   제공되는 데이터셋인 iris 데이터셋 세팅

-   훈련 데이터셋과 테스트 데이터셋 분리

```python
iris_dataset= datasets.load_iris()
x=iris_dataset.data
y=iris_dataset.target

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=1)

```

-   Classifier 설정 (층당 뉴런 수 5개, 은닉층 2층 / 최대반복 100회)
-   해당 Classifier 훈련 후 테스트셋으로 시험 후 accuracy score 출력

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    5, 2), random_state=3, max_iter=100)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 5, 은닉층 수 2, 반복횟수 100회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

```

    뉴런 수 5, 은닉층 수 2, 반복횟수 100회
    [0 2 2 0 0 2 2 0 0 2 2 0 2 2 2 2 2 2 0 0 2 2 2 0 2 2 0 0 2 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.5


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
      warnings.warn(

-   뉴런5개, 층2개 반복100회의 설정으로는 정확도가 0.5밖에 되지 않음

-   다른 설정은 두고 뉴런의 수만 2배로 늘렸을때의 결과

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    10, 2), random_state=3, max_iter=100)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 10, 은닉층 수 2, 반복횟수 100회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

```

    뉴런 수 10, 은닉층 수 2, 반복횟수 100회
    [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.2


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
      warnings.warn(

-   오히려 정확도가 0.2로 줄어든 상태.
-   무작정 늘린 뉴런은 오히려 성능을 더 저하시킬 수 있음.

-   뉴런의 수는 5개이고 층 수를 2개에서 5개로 늘렸을때의 결과

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    5, 5), random_state=3, max_iter=100)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 5, 은닉층 수 5, 반복횟수 100회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

```

    뉴런 수 5, 은닉층 수 5, 반복횟수 100회
    [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.2


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
      warnings.warn(

-   뉴런을 늘렸을때와 비슷하게 성능이 오히려 더 하락함.

-   반복 횟수를 증가시켰을 때의 결과

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    5, 2), random_state=3, max_iter=1000)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 5, 은닉층 수 2, 반복횟수 1000회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

```

    뉴런 수 5, 은닉층 수 2, 반복횟수 1000회
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 2 1 1 0 2 1 0 0 1 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.9666666666666667


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.
      warnings.warn(

-   반복횟수를 늘렸을 때 성능이 비약적으로 상승.

-   반복횟수가 늘었을 때 뉴런의 수, 은닉층의 수 중 하나만 늘어났을 때의 결과는?

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    10, 2), random_state=3, max_iter=1000)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 10, 은닉층 수 2, 반복횟수 1000회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

classifier = MLPClassifier(hidden_layer_sizes=(
    5, 5), random_state=3, max_iter=1000)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 5, 은닉층 수 5, 반복횟수 1000회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))

```

    뉴런 수 10, 은닉층 수 2, 반복횟수 1000회
    [0 2 2 0 2 2 2 0 0 2 2 0 2 2 2 0 2 2 0 0 2 2 2 0 2 2 0 0 2 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.5666666666666667
    뉴런 수 5, 은닉층 수 5, 반복횟수 1000회
    [0 1 2 0 2 2 2 0 0 2 1 0 2 2 2 0 1 2 0 0 1 2 2 0 2 1 0 0 2 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    0.7333333333333333


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.
      warnings.warn(

-   뉴런5, 층2개일때보다 성능이 하락하는 모습을 볼 수 있음.
-   뉴런과 층수를 둘 다 같이 증가시켰을 때는?

```python
classifier = MLPClassifier(hidden_layer_sizes=(
    10, 5), random_state=3, max_iter=1000)

classifier.fit(x_train, y_train)
predicted = classifier.predict(x_test)

print('뉴런 수 10, 은닉층 수 5, 반복횟수 1000회')
print(predicted)
print(y_test)
print(accuracy_score(y_test, predicted))
```

    뉴런 수 10, 은닉층 수 5, 반복횟수 1000회
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2]
    1.0


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.
      warnings.warn(

-   뉴런의 수와 은닉층의 수를 조화롭게 상승시킬 경우 성능이 상승하는것을 확인 가능.

-   여러가지를 시도해보며 데이터에 알맞는 뉴런의 수와 은닉층의 수, 반복횟수를 찾는것이 중요.

---

---

### MLP Regressor

#### 20171181 최영빈

```python
from sklearn.neural_network import MLPRegressor
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import mean_squared_error

```

-   테스트에 사용할 boston dataset load 후 테스트셋, 훈련셋 분리

```python
boston_dataset = datasets.load_boston()
x = boston_dataset.data
y = boston_dataset.target

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=0)
```

-   뉴런 수 5, 은닉층 2개로 100회 반복으로 훈련 후 결과의 squared error측정 후 출력

```python
classifier = MLPRegressor(hidden_layer_sizes=(
    5, 2), max_iter=100, random_state=1)
classifier.fit(x_train, y_train)

predicted = classifier.predict(x_test)
mse = mean_squared_error(y_test, predicted)
rmse = np.sqrt(mse)

print('뉴런 수 5, 은닉층 수 2, 반복횟수 100회')
print("RMSE : ", rmse)

```

    뉴런 수 5, 은닉층 수 2, 반복횟수 100회
    RMSE :  30.685723271739235


    /opt/homebrew/Caskroom/miniforge/base/envs/tfcv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
      warnings.warn(

-   위의 Classifier처럼 뉴런과 은닉층을 조화롭게 증가시키고, 반복횟수를 증가시킨 결과

```python
classifier = MLPRegressor(hidden_layer_sizes=(
    10, 5), max_iter=1000, random_state=1)
classifier.fit(x_train, y_train)

predicted = classifier.predict(x_test)
mse = mean_squared_error(y_test, predicted)
rmse = np.sqrt(mse)
# 결과 출력

print('뉴런 수 10, 은닉층 수 5, 반복횟수 1000회')
print("RMSE : ", rmse)

```

    뉴런 수 10, 은닉층 수 5, 반복횟수 1000회
    RMSE :  10.062802851376716

-   성능이 확실하게 증가하는 것을 확인할 수 있다.
